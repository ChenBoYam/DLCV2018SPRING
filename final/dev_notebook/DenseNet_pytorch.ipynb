{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.feature\n",
    "import pandas as pd\n",
    "% matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter \n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "train_img_folder = \"../dlcv_final_2_dataset/train/\"\n",
    "train_img_path = sorted(os.listdir(train_img_folder))\n",
    "train_X_img = [skimage.io.imread(os.path.join(train_img_folder, path))\n",
    "              for path in train_img_path]\n",
    "with open(\"../dlcv_final_2_dataset/train_id.txt\",\"r\") as f:\n",
    "    train_y = f.readlines()\n",
    "    train_y = [line.strip().split(\" \")[1] for line in train_y]\n",
    "    \n",
    "    \n",
    "valid_img_folder = \"../dlcv_final_2_dataset/val/\"\n",
    "valid_img_path = sorted(os.listdir(valid_img_folder))\n",
    "valid_X_img = [skimage.io.imread(os.path.join(valid_img_folder, path))\n",
    "              for path in valid_img_path]\n",
    "\n",
    "with open(\"../dlcv_final_2_dataset/val_id.txt\",\"r\") as f:\n",
    "    valid_y = f.readlines()\n",
    "    valid_y = [line.strip().split(\" \")[1] for line in valid_y]\n",
    "\n",
    "    \n",
    "test_img_folder = \"../dlcv_final_2_dataset/test/\"\n",
    "test_img_path = sorted(os.listdir(test_img_folder))\n",
    "test_X_img = [skimage.io.imread(os.path.join(test_img_folder, path))\n",
    "              for path in test_img_path]\n",
    "\n",
    "with open(\"../features/train_y.pkl\", \"rb\") as f:\n",
    "    train_y = pickle.load(f)\n",
    "    \n",
    "\n",
    "with open(\"../features/valid_y.pkl\", \"rb\") as f:\n",
    "    valid_y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    '''\n",
    "    normalize for pre-defined model input\n",
    "    '''\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform_input = transforms.Compose([\n",
    "             transforms.ToPILImage(),\n",
    "            transforms.Pad((23,3), fill=0, padding_mode='constant'),\n",
    "#              transforms.CenterCrop((120,90)),\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "#             normalize\n",
    "        ])\n",
    "    return transform_input(image)\n",
    "\n",
    "def normalize_flip_aug(image):\n",
    "    '''\n",
    "    normalize for pre-defined model input\n",
    "    '''\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform_input = transforms.Compose([\n",
    "             transforms.ToPILImage(),\n",
    "            transforms.Pad((23,3), fill=0, padding_mode='constant'),\n",
    "#              transforms.CenterCrop((120,90)),\n",
    "            transforms.RandomHorizontalFlip(1),\n",
    "            transforms.ToTensor(),\n",
    "#             normalize\n",
    "        ])\n",
    "    return transform_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = [normalize(img) for img in train_X_img] + [normalize_flip_aug(img) for img in train_X_img]\n",
    "train_X = torch.stack(train_X)\n",
    "\n",
    "valid_X = [normalize(img) for img in valid_X_img] + [normalize_flip_aug(img) for img in valid_X_img]\n",
    "valid_X = torch.stack(valid_X)\n",
    "\n",
    "test_X = [normalize(img) for img in test_X_img] + [normalize_flip_aug(img) for img in test_X_img]\n",
    "test_X = torch.stack(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train_y + train_y\n",
    "valid_y = valid_y + valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = {}\n",
    "label_set = []\n",
    "index = 0\n",
    "for i in train_y:\n",
    "    if i not in label_set:\n",
    "        label_set.append(i)\n",
    "        label_encoder[i] = index\n",
    "        index += 1\n",
    "output_size = 2360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thtang/.local/lib/python3.5/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "model = DenseNet121(out_size=output_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,betas=(0.5,0.999))\n",
    "BATCH_SIZE = 64\n",
    "loss_function = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 2.6868214337155223\n",
      "validation accuracy:  0.6275828595201776\n",
      "Epoch: 402\n",
      "training loss 2.39652394130826\n",
      "validation accuracy:  0.6780612952433782\n",
      "Epoch: 403\n",
      "training loss 1.9578396053984761\n",
      "validation accuracy:  0.6879073637498266\n",
      "Epoch: 404\n",
      "training loss 3.68998159840703\n",
      "validation accuracy:  0.6870753016225212\n",
      "Epoch: 405\n",
      "training loss 2.1343359649181366\n",
      "validation accuracy:  0.682429621411732\n",
      "Epoch: 406\n",
      "training loss 3.037007248029113\n",
      "validation accuracy:  0.6757037858826792\n",
      "Epoch: 407\n",
      "training loss 2.1978342812508345\n",
      "validation accuracy:  0.691790320343919\n",
      "Epoch: 408\n",
      "training loss 3.196839727461338\n",
      "validation accuracy:  0.6817362363056442\n",
      "Epoch: 409\n",
      "training loss 2.6039224565029144\n",
      "validation accuracy:  0.6865205935376508\n",
      "Epoch: 410\n",
      "training loss 2.881478387862444\n",
      "validation accuracy:  0.6825682984329496\n",
      "Epoch: 411\n",
      "training loss 2.4232459645718336\n",
      "validation accuracy:  0.6797254194979891\n",
      "Epoch: 412\n",
      "training loss 2.448295058682561\n",
      "validation accuracy:  0.6899181805574817\n",
      "Epoch: 413\n",
      "training loss 3.2810518611222506\n",
      "validation accuracy:  0.6924143669393982\n",
      "Epoch: 414\n",
      "training loss 1.7624511029571295\n",
      "validation accuracy:  0.682429621411732\n",
      "Epoch: 415\n",
      "training loss 2.861923871561885\n",
      "validation accuracy:  0.6786160033282486\n",
      "Epoch: 416\n",
      "training loss 2.7897416073828936\n",
      "validation accuracy:  0.6579531271668284\n",
      "Epoch: 417\n",
      "training loss 2.1880350895226\n",
      "validation accuracy:  0.6827069754541673\n",
      "Epoch: 418\n",
      "training loss 2.2910158345475793\n",
      "validation accuracy:  0.6775065871585079\n",
      "Epoch: 419\n",
      "training loss 3.2247964004054666\n",
      "validation accuracy:  0.6802801275828595\n",
      "Epoch: 420\n",
      "training loss 3.051299948245287\n",
      "validation accuracy:  0.6830536680072112\n",
      "Epoch: 421\n",
      "training loss 1.5820414815098047\n",
      "validation accuracy:  0.690472888642352\n",
      "Epoch: 422\n",
      "training loss 2.367502157576382\n",
      "validation accuracy:  0.6770212175842463\n",
      "Epoch: 423\n",
      "training loss 3.1991626219823956\n",
      "validation accuracy:  0.6797254194979891\n",
      "Epoch: 424\n",
      "training loss 1.9194959308952093\n",
      "validation accuracy:  0.6933157675773124\n",
      "Epoch: 425\n",
      "training loss 3.2643125522881746\n",
      "validation accuracy:  0.65843849674109\n",
      "Epoch: 426\n",
      "training loss 2.4748988654464483\n",
      "validation accuracy:  0.6819442518374705\n",
      "Epoch: 427\n",
      "training loss 2.074393019080162\n",
      "validation accuracy:  0.6838857301345167\n",
      "Epoch: 428\n",
      "training loss 2.026830056682229\n",
      "validation accuracy:  0.6781999722645957\n",
      "Epoch: 429\n",
      "training loss 2.2934301402419806\n",
      "validation accuracy:  0.674109000138677\n",
      "Epoch: 430\n",
      "training loss 3.340137779712677\n",
      "validation accuracy:  0.6869366246013036\n",
      "Epoch: 431\n",
      "training loss 2.273125469684601\n",
      "validation accuracy:  0.672444875884066\n",
      "Epoch: 432\n",
      "training loss 3.004926336929202\n",
      "validation accuracy:  0.6786853418388573\n",
      "Epoch: 433\n",
      "training loss 2.2225556783378124\n",
      "validation accuracy:  0.6798640965192068\n",
      "Epoch: 434\n",
      "training loss 2.036699863150716\n",
      "validation accuracy:  0.6749410622659825\n",
      "Epoch: 435\n",
      "training loss 1.4039481598883867\n",
      "validation accuracy:  0.6902648731105255\n",
      "Epoch: 436\n",
      "training loss 3.063071520999074\n",
      "validation accuracy:  0.6899875190680904\n",
      "Epoch: 437\n",
      "training loss 2.2326587988063693\n",
      "validation accuracy:  0.6745943697129385\n",
      "Epoch: 438\n",
      "training loss 2.449061945080757\n",
      "validation accuracy:  0.6765358480099848\n",
      "Epoch: 439\n",
      "training loss 1.7739081140607595\n",
      "validation accuracy:  0.6872833171543475\n",
      "Epoch: 440\n",
      "training loss 3.2037766464054585\n",
      "validation accuracy:  0.6834003605602552\n",
      "Epoch: 441\n",
      "training loss 1.9493557810783386\n",
      "validation accuracy:  0.68312300651782\n",
      "Epoch: 442\n",
      "training loss 2.475567327812314\n",
      "validation accuracy:  0.6804188046040771\n",
      "Epoch: 443\n",
      "training loss 1.9421202931553125\n",
      "validation accuracy:  0.6940784911940091\n",
      "Epoch: 444\n",
      "training loss 2.979628909379244\n",
      "validation accuracy:  0.6834003605602552\n",
      "Epoch: 445\n",
      "training loss 2.296911971643567\n",
      "validation accuracy:  0.6913742892802662\n",
      "Epoch: 446\n",
      "training loss 2.0260716900229454\n",
      "validation accuracy:  0.6814588822632089\n",
      "Epoch: 447\n",
      "training loss 2.8365853670984507\n",
      "validation accuracy:  0.6894328109832201\n",
      "Epoch: 448\n",
      "training loss 1.3143116794526577\n",
      "validation accuracy:  0.6974067397032312\n",
      "Epoch: 449\n",
      "training loss 3.674876421689987\n",
      "validation accuracy:  0.683469699070864\n",
      "Epoch: 450\n",
      "training loss 1.3932738304138184\n",
      "validation accuracy:  0.695465261406185\n",
      "Epoch: 451\n",
      "training loss 1.4908893629908562\n",
      "validation accuracy:  0.6716821522673693\n",
      "Epoch: 452\n",
      "training loss 3.2891457565128803\n",
      "validation accuracy:  0.683816391623908\n",
      "Epoch: 453\n",
      "training loss 2.5357720321044326\n",
      "validation accuracy:  0.6763971709887672\n",
      "Epoch: 454\n",
      "training loss 3.8550766725093126\n",
      "validation accuracy:  0.6843710997087783\n",
      "Epoch: 455\n",
      "training loss 2.251088136807084\n",
      "validation accuracy:  0.6858965469421717\n",
      "Epoch: 456\n",
      "training loss 3.151072032749653\n",
      "validation accuracy:  0.6840244071557343\n",
      "Epoch: 457\n",
      "training loss 1.4837015569210052\n",
      "validation accuracy:  0.6977534322562752\n",
      "Epoch: 458\n",
      "training loss 2.5833713095635176\n",
      "validation accuracy:  0.6331992788794897\n",
      "Epoch: 459\n",
      "training loss 2.1307886987924576\n",
      "validation accuracy:  0.6889474414089586\n",
      "Epoch: 460\n",
      "training loss 2.2346242927014828\n",
      "validation accuracy:  0.6905422271529608\n",
      "Epoch: 461\n",
      "training loss 1.602779159322381\n",
      "validation accuracy:  0.695118568853141\n",
      "Epoch: 462\n",
      "training loss 2.1574895437806845\n",
      "validation accuracy:  0.6796560809873804\n",
      "Epoch: 463\n",
      "training loss 2.2464601574465632\n",
      "validation accuracy:  0.6831923450284287\n",
      "Epoch: 464\n",
      "training loss 2.9741276260465384\n",
      "validation accuracy:  0.6879767022604354\n",
      "Epoch: 465\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "model.train()\n",
    "training_loss_list = []\n",
    "validation_acc_list = []\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"../log/densenet_flip_01N\")\n",
    "# iteration = 0\n",
    "for epoch in range(400,500):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    CE_loss = 0.0\n",
    "    total_length = len(train_X)\n",
    "    # shuffle\n",
    "    perm_index = np.random.permutation(total_length)\n",
    "    train_X_sfl = [train_X[i] for i in perm_index]\n",
    "    train_y_sfl = [train_y[i] for i in perm_index]\n",
    "\n",
    "    # construct training batch\n",
    "    for index in range(0,total_length ,BATCH_SIZE):\n",
    "        iteration+=1\n",
    "        if index+BATCH_SIZE > total_length:\n",
    "            break\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        input_X = torch.stack(train_X_sfl[index:index+BATCH_SIZE])\n",
    "        input_y = train_y_sfl[index:index+BATCH_SIZE]\n",
    "        input_y = [label_encoder[y] for y in input_y]\n",
    "        input_y = torch.tensor(input_y).type(torch.LongTensor)\n",
    "        # use GPU\n",
    "        # forward + backward + optimize\n",
    "        output = model(input_X.cuda())\n",
    "        # compute loss for each sample in training data\n",
    "        loss = loss_function(output, input_y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        CE_loss += loss.cpu().data.numpy()\n",
    "        writer.add_scalar('loss', loss.data[0], iteration)\n",
    "    print(\"training loss\",CE_loss)\n",
    "    training_loss_list.append(CE_loss)\n",
    "    # validation\n",
    "    same_difference = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_output = []\n",
    "        valid_y_list = []\n",
    "        for X, y in zip(valid_X, valid_y):\n",
    "            input_valid_X = X.unsqueeze(0)\n",
    "            output = model(input_valid_X.cuda(), )\n",
    "            prediction = torch.argmax(torch.squeeze(output.cpu())).data.numpy()\n",
    "            valid_gt = label_encoder[y]\n",
    "            same_difference.append(prediction==valid_gt)\n",
    "\n",
    "        accuracy = np.mean(same_difference)\n",
    "        validation_acc_list.append(accuracy)\n",
    "        print(\"validation accuracy: \",accuracy)\n",
    "        writer.add_scalar('accuracy', accuracy,  epoch+1)\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "#         torch.save(model.state_dict(), \"../models/RNN_seq2seq_model.pkt\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_output = []\n",
    "        for X in test_X:\n",
    "            input_test_X = X.unsqueeze(0)\n",
    "            output = model(input_test_X.cuda())\n",
    "            prediction = int(torch.argmax(torch.squeeze(output.cpu())).data.numpy())\n",
    "            test_output.append(label_encoder_inv[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder_inv ={v: k for k, v in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load SampleSubmission\n",
    "submission = pd.read_csv(\"../SampleSubmission.csv\")\n",
    "submission[\"ans\"] = test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"test_output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
