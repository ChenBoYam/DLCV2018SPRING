{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.load(\"all_img.npy\")\n",
    "train_X = torch.from_numpy(train_X).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 92, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# fixed input: two 10-dimensional data\n",
    "z = torch.randn((1, 72)).repeat(100,1)\n",
    "c = []\n",
    "for i in range(10):\n",
    "    c_1 = torch.zeros(10,10)\n",
    "    c_1[:,i] = 1\n",
    "    c_2 = torch.zeros(10,10)\n",
    "    c_2[[range(10)],[range(10)]] = 1\n",
    "    c_ = torch.cat((c_1, c_2),1)\n",
    "    c.append(c_)\n",
    "c = torch.cat(c)\n",
    "fixed_input = torch.cat((z, c),1).view(100,92,1,1)\n",
    "print(fixed_input.size())\n",
    "fixed_input = Variable(fixed_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, figsize=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = 72+20\n",
    "        self.figsize = figsize\n",
    "        self.decoder = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            \n",
    "            nn.ConvTranspose2d( self.noise_dim, self.figsize * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(self.figsize * 8, self.figsize * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(self.figsize * 4, self.figsize * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.figsize * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(self.figsize * 2, self.figsize, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.figsize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(self.figsize, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.decoder(X)/2.0+0.5\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        figsize = 64\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(3, figsize, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(figsize, figsize * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(figsize * 2, figsize * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(figsize * 4, figsize * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "                    \n",
    "        self.D_out = nn.Sequential(\n",
    "            nn.Conv2d(figsize * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.Q_out = nn.Sequential(\n",
    "            nn.Linear(8192, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_decoder(x)\n",
    "        encode_output_size = x.size()\n",
    "#         print(\"conv encode size:\",x.size()) # (batch_size, 512,4,4)\n",
    "        real_fake = self.D_out(x).view(-1, 1)\n",
    "        x = x.view(-1, 512*4*4)\n",
    "        discrete = self.Q_out(x)\n",
    "        return real_fake, discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thtang/.local/lib/python3.5/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training D Loss: 0.021318719933916326\n",
      "training G Loss: 0.012957220859712858\n",
      "training Info Loss: 0.006141772730806231\n",
      "D_real_acc: 0.9784847844959058\n",
      "D_fake_acc: 0.06011121278243119\n",
      "Epoch: 2\n",
      "training D Loss: 0.02217619225436297\n",
      "training G Loss: 0.010677009041285134\n",
      "training Info Loss: 0.004974195961575067\n",
      "D_real_acc: 0.9810891344642313\n",
      "D_fake_acc: 0.011919007062246311\n",
      "Epoch: 3\n",
      "training D Loss: 0.022112462217464695\n",
      "training G Loss: 0.0103468227156005\n",
      "training Info Loss: 0.004215199323676752\n",
      "D_real_acc: 0.9809483587902678\n",
      "D_fake_acc: 0.02106942586987635\n",
      "Epoch: 4\n",
      "training D Loss: 0.022172785986174934\n",
      "training G Loss: 0.010191022045678734\n",
      "training Info Loss: 0.0030382949369348244\n",
      "D_real_acc: 0.9828488303887755\n",
      "D_fake_acc: 0.020389010112385912\n",
      "Epoch: 5\n",
      "training D Loss: 0.02204310897368805\n",
      "training G Loss: 0.010173230778032347\n",
      "training Info Loss: 0.0019389892562971063\n",
      "D_real_acc: 0.9825438164285212\n",
      "D_fake_acc: 0.01987283264118627\n",
      "Epoch: 6\n",
      "training D Loss: 0.022119849297249687\n",
      "training G Loss: 0.0100240795998204\n",
      "training Info Loss: 0.0010835552272878918\n",
      "D_real_acc: 0.984631988925647\n",
      "D_fake_acc: 0.018230449778278313\n",
      "Epoch: 7\n",
      "training D Loss: 0.022061964825656838\n",
      "training G Loss: 0.009995706522583064\n",
      "training Info Loss: 0.0006506995250485599\n",
      "D_real_acc: 0.9856878064803736\n",
      "D_fake_acc: 0.017972361042678492\n",
      "Epoch: 8\n",
      "training D Loss: 0.02191504645399619\n",
      "training G Loss: 0.010102796363253274\n",
      "training Info Loss: 0.00047814945549458766\n",
      "D_real_acc: 0.984631988925647\n",
      "D_fake_acc: 0.020248234438422375\n",
      "Epoch: 9\n",
      "training D Loss: 0.02190234628321908\n",
      "training G Loss: 0.010170691981812018\n",
      "training Info Loss: 0.0003611753007469121\n",
      "D_real_acc: 0.9858755073789915\n",
      "D_fake_acc: 0.020365547500058655\n",
      "Epoch: 10\n",
      "training D Loss: 0.02195149048554875\n",
      "training G Loss: 0.010119115151719288\n",
      "training Info Loss: 0.0003061517928645138\n",
      "D_real_acc: 0.9851481663968467\n",
      "D_fake_acc: 0.021538678116421482\n",
      "Epoch: 11\n",
      "training D Loss: 0.022066542718577248\n",
      "training G Loss: 0.009925371679627927\n",
      "training Info Loss: 0.0002492170519162742\n",
      "D_real_acc: 0.9850543159475376\n",
      "D_fake_acc: 0.018840477698786984\n",
      "Epoch: 12\n",
      "training D Loss: 0.022185843644972624\n",
      "training G Loss: 0.009875065868484462\n",
      "training Info Loss: 0.0002203526058534314\n",
      "D_real_acc: 0.9854531803571009\n",
      "D_fake_acc: 0.017596959245442387\n",
      "Epoch: 13\n",
      "training D Loss: 0.02222530966180294\n",
      "training G Loss: 0.00972378942268955\n",
      "training Info Loss: 0.0002021083217197682\n",
      "D_real_acc: 0.9850543159475376\n",
      "D_fake_acc: 0.01792543581802398\n",
      "Epoch: 14\n",
      "training D Loss: 0.022252985479651516\n",
      "training G Loss: 0.00967937242906177\n",
      "training Info Loss: 0.0001696749985604728\n",
      "D_real_acc: 0.9853124046831374\n",
      "D_fake_acc: 0.018160061941296542\n",
      "Epoch: 15\n",
      "training D Loss: 0.022275477118456437\n",
      "training G Loss: 0.009663543342377673\n",
      "training Info Loss: 0.0001686833992837134\n",
      "D_real_acc: 0.9852185542338284\n",
      "D_fake_acc: 0.015790338096243636\n",
      "Epoch: 16\n",
      "training D Loss: 0.022251439190758927\n",
      "training G Loss: 0.00972125677698722\n",
      "training Info Loss: 0.0001480458858488598\n",
      "D_real_acc: 0.9839515731681565\n",
      "D_fake_acc: 0.0181835245536238\n",
      "Epoch: 17\n",
      "training D Loss: 0.022188372615467744\n",
      "training G Loss: 0.00974205630032735\n",
      "training Info Loss: 0.00013539245232043555\n",
      "D_real_acc: 0.9848900776612468\n",
      "D_fake_acc: 0.01914549165904132\n",
      "Epoch: 18\n",
      "training D Loss: 0.022109511890336643\n",
      "training G Loss: 0.009857205701191503\n",
      "training Info Loss: 0.0001344665403664829\n",
      "D_real_acc: 0.9855001055817555\n",
      "D_fake_acc: 0.01947396823162291\n",
      "Epoch: 19\n",
      "training D Loss: 0.02213294455000484\n",
      "training G Loss: 0.00977379102512912\n",
      "training Info Loss: 0.00012370559408716566\n",
      "D_real_acc: 0.9857112690927008\n",
      "D_fake_acc: 0.019497430843950166\n",
      "Epoch: 20\n",
      "training D Loss: 0.022121989769813773\n",
      "training G Loss: 0.009776872643021258\n",
      "training Info Loss: 0.00011860663098261064\n",
      "D_real_acc: 0.9844912132516834\n",
      "D_fake_acc: 0.019614743905586447\n",
      "Epoch: 21\n",
      "training D Loss: 0.022027669576440517\n",
      "training G Loss: 0.009868495074876053\n",
      "training Info Loss: 0.00011294113138828647\n",
      "D_real_acc: 0.9834119330846296\n",
      "D_fake_acc: 0.02566809788601863\n",
      "Epoch: 22\n",
      "training D Loss: 0.02199741677545835\n",
      "training G Loss: 0.009823594736959498\n",
      "training Info Loss: 0.00010598373547300749\n",
      "D_real_acc: 0.9850308533352103\n",
      "D_fake_acc: 0.021796766852021303\n",
      "Epoch: 23\n",
      "training D Loss: 0.021892098300754205\n",
      "training G Loss: 0.009958383460620028\n",
      "training Info Loss: 0.00010814239739508928\n",
      "D_real_acc: 0.98554703080641\n",
      "D_fake_acc: 0.025574247436709604\n",
      "Epoch: 24\n",
      "training D Loss: 0.02182462036921511\n",
      "training G Loss: 0.01002741961708426\n",
      "training Info Loss: 0.0001067390651793151\n",
      "D_real_acc: 0.9835996339832477\n",
      "D_fake_acc: 0.02970366720630675\n",
      "Epoch: 25\n",
      "training D Loss: 0.02174029446272475\n",
      "training G Loss: 0.01012154596425443\n",
      "training Info Loss: 0.00010457806347615359\n",
      "D_real_acc: 0.9846789141503015\n",
      "D_fake_acc: 0.03160413880481453\n",
      "Epoch: 26\n",
      "training D Loss: 0.021653840832220127\n",
      "training G Loss: 0.010155357260842882\n",
      "training Info Loss: 0.00010118970056607103\n",
      "D_real_acc: 0.9850308533352103\n",
      "D_fake_acc: 0.032659956359541074\n",
      "Epoch: 27\n",
      "training D Loss: 0.021540014222059763\n",
      "training G Loss: 0.010267238475305238\n",
      "training Info Loss: 9.843105245083761e-05\n",
      "D_real_acc: 0.9834588583092841\n",
      "D_fake_acc: 0.0417634499425166\n",
      "Epoch: 28\n",
      "training D Loss: 0.021361017025232243\n",
      "training G Loss: 0.01050850447573299\n",
      "training Info Loss: 0.00010611679783350411\n",
      "D_real_acc: 0.9806902700546679\n",
      "D_fake_acc: 0.05344783088149035\n",
      "Epoch: 29\n",
      "training D Loss: 0.021241978386684096\n",
      "training G Loss: 0.01055601142079924\n",
      "training Info Loss: 0.00010408647705579413\n",
      "D_real_acc: 0.9835996339832477\n",
      "D_fake_acc: 0.05659182093334272\n",
      "Epoch: 30\n",
      "training D Loss: 0.02115871500105791\n",
      "training G Loss: 0.010634936441079296\n",
      "training Info Loss: 0.00010228841881764427\n",
      "D_real_acc: 0.981253372750522\n",
      "D_fake_acc: 0.06259824968912038\n",
      "Epoch: 31\n",
      "training D Loss: 0.021035483295110142\n",
      "training G Loss: 0.010742704312728248\n",
      "training Info Loss: 0.00010335097241363494\n",
      "D_real_acc: 0.9822388024682668\n",
      "D_fake_acc: 0.0702001360831515\n",
      "Epoch: 32\n",
      "training D Loss: 0.020919569203615363\n",
      "training G Loss: 0.010903071741566543\n",
      "training Info Loss: 0.00010722408414050297\n",
      "D_real_acc: 0.9825438164285212\n",
      "D_fake_acc: 0.07447033152671219\n",
      "Epoch: 33\n",
      "training D Loss: 0.020811831732139283\n",
      "training G Loss: 0.011033133894232364\n",
      "training Info Loss: 9.711943679715552e-05\n",
      "D_real_acc: 0.9834588583092841\n",
      "D_fake_acc: 0.08122756387696206\n",
      "Epoch: 34\n",
      "training D Loss: 0.020692760220904435\n",
      "training G Loss: 0.011165801802373152\n",
      "training Info Loss: 0.00011581867344114166\n",
      "D_real_acc: 0.9791417376410689\n",
      "D_fake_acc: 0.09403815020764413\n",
      "Epoch: 35\n",
      "training D Loss: 0.02052695552438112\n",
      "training G Loss: 0.01127344987597219\n",
      "training Info Loss: 0.00010054441911213476\n",
      "D_real_acc: 0.9814879988737946\n",
      "D_fake_acc: 0.10168696182632975\n",
      "Epoch: 36\n",
      "training D Loss: 0.020227843449045886\n",
      "training G Loss: 0.011540956274106746\n",
      "training Info Loss: 0.00010757117063058023\n",
      "D_real_acc: 0.9817695502217216\n",
      "D_fake_acc: 0.12228713544966097\n",
      "Epoch: 37\n",
      "training D Loss: 0.020061942413985956\n",
      "training G Loss: 0.011814780524048817\n",
      "training Info Loss: 0.00010452730714697413\n",
      "D_real_acc: 0.9815349240984491\n",
      "D_fake_acc: 0.13744398301306868\n",
      "Epoch: 38\n",
      "training D Loss: 0.01984315529262546\n",
      "training G Loss: 0.011992324837906397\n",
      "training Info Loss: 0.00011710441096656481\n",
      "D_real_acc: 0.9827549799394665\n",
      "D_fake_acc: 0.15574482062832876\n",
      "Epoch: 39\n",
      "training D Loss: 0.019703645475819022\n",
      "training G Loss: 0.01214150813991865\n",
      "training Info Loss: 0.00011598032115767673\n",
      "D_real_acc: 0.9820276389573215\n",
      "D_fake_acc: 0.16496562727294056\n",
      "Epoch: 40\n",
      "training D Loss: 0.019556382753465232\n",
      "training G Loss: 0.012347655081127477\n",
      "training Info Loss: 0.00010775460981848923\n",
      "D_real_acc: 0.9833650078599752\n",
      "D_fake_acc: 0.1751014757983154\n",
      "Epoch: 41\n",
      "training D Loss: 0.01939098013132333\n",
      "training G Loss: 0.012534917214825176\n",
      "training Info Loss: 0.0001173748555342668\n",
      "D_real_acc: 0.9822622650805941\n",
      "D_fake_acc: 0.19584242509561015\n",
      "Epoch: 42\n",
      "training D Loss: 0.019324653938121432\n",
      "training G Loss: 0.01260706197706158\n",
      "training Info Loss: 0.0001150620161355528\n",
      "D_real_acc: 0.9842096619037564\n",
      "D_fake_acc: 0.1968982426503367\n",
      "Epoch: 43\n",
      "training D Loss: 0.019154734517085815\n",
      "training G Loss: 0.012764034731116619\n",
      "training Info Loss: 0.00011266645558619995\n",
      "D_real_acc: 0.9835527087585931\n",
      "D_fake_acc: 0.20942727763309166\n",
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training D Loss: 0.01900921568055286\n",
      "training G Loss: 0.01291845616695538\n",
      "training Info Loss: 0.00011852257913664004\n",
      "D_real_acc: 0.9838811853311747\n",
      "D_fake_acc: 0.2303559278290045\n",
      "Epoch: 45\n",
      "training D Loss: 0.01888232523450567\n",
      "training G Loss: 0.013050789334762215\n",
      "training Info Loss: 0.00012115446401297216\n",
      "D_real_acc: 0.9835057835339387\n",
      "D_fake_acc: 0.23486074939583773\n",
      "Epoch: 46\n",
      "training D Loss: 0.018687899684232498\n",
      "training G Loss: 0.013169788874587696\n",
      "training Info Loss: 0.00011853917950921146\n",
      "D_real_acc: 0.9839984983928111\n",
      "D_fake_acc: 0.24884446634288263\n",
      "Epoch: 47\n",
      "training D Loss: 0.018564921838887426\n",
      "training G Loss: 0.013415771686013538\n",
      "training Info Loss: 0.00012720938193254335\n",
      "D_real_acc: 0.9847493019872833\n",
      "D_fake_acc: 0.25977804368738416\n",
      "Epoch: 48\n",
      "training D Loss: 0.018440007517354078\n",
      "training G Loss: 0.013640830370869743\n",
      "training Info Loss: 0.00012367110866817713\n",
      "D_real_acc: 0.9842331245160836\n",
      "D_fake_acc: 0.27312827010159313\n",
      "Epoch: 49\n",
      "training D Loss: 0.018352269815880062\n",
      "training G Loss: 0.013651005024000512\n",
      "training Info Loss: 0.00013340759008295734\n",
      "D_real_acc: 0.984631988925647\n",
      "D_fake_acc: 0.28087093216958775\n",
      "Epoch: 50\n",
      "learning rate change!\n",
      "training D Loss: 0.017028896870045537\n",
      "training G Loss: 0.01251701565530125\n",
      "training Info Loss: 0.00011084168063260048\n",
      "D_real_acc: 0.9911076699279697\n",
      "D_fake_acc: 0.37941390394406516\n",
      "Epoch: 51\n",
      "training D Loss: 0.016707061466815276\n",
      "training G Loss: 0.013052566772931365\n",
      "training Info Loss: 0.00011205994009437181\n",
      "D_real_acc: 0.9905914924567701\n",
      "D_fake_acc: 0.40810867882029983\n",
      "Epoch: 52\n",
      "training D Loss: 0.016543638440216252\n",
      "training G Loss: 0.013343257561257153\n",
      "training Info Loss: 0.00010739399165400008\n",
      "D_real_acc: 0.9903099411088431\n",
      "D_fake_acc: 0.42450904483705215\n",
      "Epoch: 53\n",
      "training D Loss: 0.016355443953438492\n",
      "training G Loss: 0.01357815801888947\n",
      "training Info Loss: 0.00012059025220506171\n",
      "D_real_acc: 0.9904507167828066\n",
      "D_fake_acc: 0.439126252316933\n",
      "Epoch: 54\n",
      "training D Loss: 0.016244341351481863\n",
      "training G Loss: 0.013781993735840967\n",
      "training Info Loss: 0.00011738557107558243\n",
      "D_real_acc: 0.9901691654348795\n",
      "D_fake_acc: 0.44733816663147274\n",
      "Epoch: 55\n",
      "training D Loss: 0.016152609119315486\n",
      "training G Loss: 0.013905765602151824\n",
      "training Info Loss: 0.00012810269848136056\n",
      "D_real_acc: 0.9893714366157528\n",
      "D_fake_acc: 0.4595621876539734\n",
      "Epoch: 56\n",
      "training D Loss: 0.0159881930078162\n",
      "training G Loss: 0.01410788222414938\n",
      "training Info Loss: 0.00012650311017711695\n",
      "D_real_acc: 0.9913892212758969\n",
      "D_fake_acc: 0.46995612491494804\n",
      "Epoch: 57\n",
      "training D Loss: 0.015959759006467577\n",
      "training G Loss: 0.014259660042047428\n",
      "training Info Loss: 0.0001262838983793817\n",
      "D_real_acc: 0.9894418244527345\n",
      "D_fake_acc: 0.4752117500762535\n",
      "Epoch: 58\n",
      "training D Loss: 0.015913175202689193\n",
      "training G Loss: 0.014291064000997318\n",
      "training Info Loss: 0.00012963280682145122\n",
      "D_real_acc: 0.9891133478801529\n",
      "D_fake_acc: 0.4768775955514887\n",
      "Epoch: 59\n",
      "training D Loss: 0.01586621519222641\n",
      "training G Loss: 0.014479511836533846\n",
      "training Info Loss: 0.00012851856941543122\n",
      "D_real_acc: 0.9885033199596444\n",
      "D_fake_acc: 0.4865207292179911\n",
      "Epoch: 60\n",
      "training D Loss: 0.015855367920237104\n",
      "training G Loss: 0.014489841611937858\n",
      "training Info Loss: 0.00012860437295950718\n",
      "D_real_acc: 0.9895122122897163\n",
      "D_fake_acc: 0.4817578189155581\n",
      "Epoch: 61\n",
      "training D Loss: 0.015743812700010573\n",
      "training G Loss: 0.014681191290675978\n",
      "training Info Loss: 0.00013628595307537894\n",
      "D_real_acc: 0.9888317965322259\n",
      "D_fake_acc: 0.49271485887238686\n",
      "Epoch: 62\n",
      "training D Loss: 0.01566342628700568\n",
      "training G Loss: 0.014788464490342702\n",
      "training Info Loss: 0.00013182554391603091\n",
      "D_real_acc: 0.9892541235541165\n",
      "D_fake_acc: 0.49651580206940243\n",
      "Epoch: 63\n",
      "training D Loss: 0.015579447490410831\n",
      "training G Loss: 0.014823701431693018\n",
      "training Info Loss: 0.00013811903780113898\n",
      "D_real_acc: 0.989301048778771\n",
      "D_fake_acc: 0.5054081321414327\n",
      "Epoch: 64\n",
      "training D Loss: 0.015529839217653022\n",
      "training G Loss: 0.014941665958769175\n",
      "training Info Loss: 0.000141025899680064\n",
      "D_real_acc: 0.9888787217568804\n",
      "D_fake_acc: 0.5065343375331409\n",
      "Epoch: 65\n",
      "training D Loss: 0.01556111576242725\n",
      "training G Loss: 0.01500779344744613\n",
      "training Info Loss: 0.000137785321602989\n",
      "D_real_acc: 0.9894418244527345\n",
      "D_fake_acc: 0.5077074681495037\n",
      "Epoch: 66\n",
      "training D Loss: 0.015518846781971346\n",
      "training G Loss: 0.015101983281443427\n",
      "training Info Loss: 0.00014099530603339645\n",
      "D_real_acc: 0.9894183618404073\n",
      "D_fake_acc: 0.5108514582013561\n",
      "Epoch: 67\n",
      "training D Loss: 0.01551352157454268\n",
      "training G Loss: 0.015113153448635386\n",
      "training Info Loss: 0.00013474203419782104\n",
      "D_real_acc: 0.9897233758006616\n",
      "D_fake_acc: 0.5055958330400506\n",
      "Epoch: 68\n",
      "training D Loss: 0.015436600690344353\n",
      "training G Loss: 0.015205348956892876\n",
      "training Info Loss: 0.00013966973614152944\n",
      "D_real_acc: 0.989207198329462\n",
      "D_fake_acc: 0.5133150324957181\n",
      "Epoch: 69\n",
      "training D Loss: 0.015389532499010612\n",
      "training G Loss: 0.015224578436747701\n",
      "training Info Loss: 0.00014636402750232513\n",
      "D_real_acc: 0.989301048778771\n",
      "D_fake_acc: 0.5214096337486216\n",
      "Epoch: 70\n",
      "training D Loss: 0.015395339494219066\n",
      "training G Loss: 0.015393205872811799\n",
      "training Info Loss: 0.00014637091879444623\n",
      "D_real_acc: 0.9896529879636798\n",
      "D_fake_acc: 0.5159897703010253\n",
      "Epoch: 71\n",
      "training D Loss: 0.015357914523570888\n",
      "training G Loss: 0.015364401889416252\n",
      "training Info Loss: 0.00014459033580553938\n",
      "D_real_acc: 0.9889021843692076\n",
      "D_fake_acc: 0.5204476666432041\n",
      "Epoch: 72\n",
      "training D Loss: 0.015283611130897337\n",
      "training G Loss: 0.015506736358885042\n",
      "training Info Loss: 0.00015024376018844474\n",
      "D_real_acc: 0.9902160906595341\n",
      "D_fake_acc: 0.5218788859951667\n",
      "Epoch: 73\n",
      "training D Loss: 0.015269921136476901\n",
      "training G Loss: 0.015559340101727036\n",
      "training Info Loss: 0.00014434015105193373\n",
      "D_real_acc: 0.9905211046197884\n",
      "D_fake_acc: 0.5207761432157857\n",
      "Epoch: 74\n",
      "training D Loss: 0.01525749869277366\n",
      "training G Loss: 0.01560908335723791\n",
      "training Info Loss: 0.00013923539841493286\n",
      "D_real_acc: 0.9910607447033153\n",
      "D_fake_acc: 0.5227235400389479\n",
      "Epoch: 75\n",
      "training D Loss: 0.015244155054467648\n",
      "training G Loss: 0.01559908195768231\n",
      "training Info Loss: 0.00014842512433146558\n",
      "D_real_acc: 0.9897703010253162\n",
      "D_fake_acc: 0.525468665681237\n",
      "Epoch: 76\n",
      "training D Loss: 0.01522560909742517\n",
      "training G Loss: 0.015721364684013292\n",
      "training Info Loss: 0.00014483765629864752\n",
      "D_real_acc: 0.9904272541704794\n",
      "D_fake_acc: 0.5278618521386171\n",
      "Epoch: 77\n",
      "training D Loss: 0.015164504721975766\n",
      "training G Loss: 0.015723697503609177\n",
      "training Info Loss: 0.00015193163219034159\n",
      "D_real_acc: 0.9900987775978978\n",
      "D_fake_acc: 0.5295042350015251\n",
      "Epoch: 78\n",
      "training D Loss: 0.01508983582099497\n",
      "training G Loss: 0.01570532914242436\n",
      "training Info Loss: 0.0001486736250930966\n",
      "D_real_acc: 0.9904741793951338\n",
      "D_fake_acc: 0.5325309119917412\n",
      "Epoch: 79\n",
      "training D Loss: 0.015091447929380445\n",
      "training G Loss: 0.015881634718462056\n",
      "training Info Loss: 0.00014252086303998722\n",
      "D_real_acc: 0.9894887496773891\n",
      "D_fake_acc: 0.5357687524929026\n",
      "Epoch: 80\n",
      "learning rate change!\n",
      "training D Loss: 0.014368861813689663\n",
      "training G Loss: 0.014436194020745644\n",
      "training Info Loss: 0.00013283117210629238\n",
      "D_real_acc: 0.993688557283968\n",
      "D_fake_acc: 0.5946599094343165\n",
      "Epoch: 81\n",
      "training D Loss: 0.01412720007399795\n",
      "training G Loss: 0.014653021284719486\n",
      "training Info Loss: 0.00013562794667918072\n",
      "D_real_acc: 0.9946270617770583\n",
      "D_fake_acc: 0.6074704957649985\n",
      "Epoch: 82\n",
      "training D Loss: 0.0139940262514888\n",
      "training G Loss: 0.014695294734719621\n",
      "training Info Loss: 0.0001330618666690485\n",
      "D_real_acc: 0.9947443748386945\n",
      "D_fake_acc: 0.6154243213439384\n",
      "Epoch: 83\n",
      "training D Loss: 0.013976350942363637\n",
      "training G Loss: 0.01487665971595682\n",
      "training Info Loss: 0.00014019733541130184\n",
      "D_real_acc: 0.9946505243893855\n",
      "D_fake_acc: 0.6186621618450998\n",
      "Epoch: 84\n",
      "training D Loss: 0.013965608918631318\n",
      "training G Loss: 0.015027204878027431\n",
      "training Info Loss: 0.00013175190433226838\n",
      "D_real_acc: 0.9947443748386945\n",
      "D_fake_acc: 0.6157293353041927\n",
      "Epoch: 85\n",
      "training D Loss: 0.013887697598255127\n",
      "training G Loss: 0.015162596801364359\n",
      "training Info Loss: 0.0001409735242735421\n",
      "D_real_acc: 0.9946270617770583\n",
      "D_fake_acc: 0.6195302785012083\n",
      "Epoch: 86\n",
      "training D Loss: 0.013855794050264962\n",
      "training G Loss: 0.015157425347096352\n",
      "training Info Loss: 0.00014485121677575217\n",
      "D_real_acc: 0.9947443748386945\n",
      "D_fake_acc: 0.6208207221792075\n",
      "Epoch: 87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training D Loss: 0.013808350176677076\n",
      "training G Loss: 0.015199277566326814\n",
      "training Info Loss: 0.00013826022835528403\n",
      "D_real_acc: 0.9948851505126581\n",
      "D_fake_acc: 0.6253020811337134\n",
      "Epoch: 88\n",
      "training D Loss: 0.013712439702404695\n",
      "training G Loss: 0.015304293225213813\n",
      "training Info Loss: 0.00014168773738751104\n",
      "D_real_acc: 0.9944862861030948\n",
      "D_fake_acc: 0.6299711409868375\n",
      "Epoch: 89\n",
      "training D Loss: 0.013731447934226124\n",
      "training G Loss: 0.015449618201558877\n",
      "training Info Loss: 0.00014878891779685443\n",
      "D_real_acc: 0.9951432392482579\n",
      "D_fake_acc: 0.6309565707045822\n",
      "Epoch: 90\n",
      "training D Loss: 0.01370442706543278\n",
      "training G Loss: 0.01539914546356681\n",
      "training Info Loss: 0.00013830179371378856\n",
      "D_real_acc: 0.9945566739400765\n",
      "D_fake_acc: 0.6281175946129842\n",
      "Epoch: 91\n",
      "training D Loss: 0.013674517026469707\n",
      "training G Loss: 0.01551971129894939\n",
      "training Info Loss: 0.0001472466230961769\n",
      "D_real_acc: 0.9940404964688768\n",
      "D_fake_acc: 0.6278595058773844\n",
      "Epoch: 92\n",
      "training D Loss: 0.013606871022726336\n",
      "training G Loss: 0.015503120987069777\n",
      "training Info Loss: 0.0001431162422676954\n",
      "D_real_acc: 0.9943689730414584\n",
      "D_fake_acc: 0.6377372656671594\n",
      "Epoch: 93\n",
      "training D Loss: 0.013621670666809629\n",
      "training G Loss: 0.015601288806334589\n",
      "training Info Loss: 0.00014589538485361474\n",
      "D_real_acc: 0.9942985852044767\n",
      "D_fake_acc: 0.6319420004223271\n",
      "Epoch: 94\n",
      "training D Loss: 0.013549136785716773\n",
      "training G Loss: 0.015702543595477077\n",
      "training Info Loss: 0.00014096644958652287\n",
      "D_real_acc: 0.9940639590812042\n",
      "D_fake_acc: 0.6372914760329416\n",
      "Epoch: 95\n",
      "training D Loss: 0.013622782102332308\n",
      "training G Loss: 0.015716945823518083\n",
      "training Info Loss: 0.00014519213027926978\n",
      "D_real_acc: 0.9941108843058586\n",
      "D_fake_acc: 0.6336547711222168\n",
      "Epoch: 96\n",
      "training D Loss: 0.013542608538085886\n",
      "training G Loss: 0.01573142646157348\n",
      "training Info Loss: 0.00015108026304746784\n",
      "D_real_acc: 0.9941343469181859\n",
      "D_fake_acc: 0.6369160742357054\n",
      "Epoch: 97\n",
      "training D Loss: 0.01351564839831578\n",
      "training G Loss: 0.015818445513197596\n",
      "training Info Loss: 0.00014105077314199242\n",
      "D_real_acc: 0.9944862861030948\n",
      "D_fake_acc: 0.6401773773491941\n",
      "Epoch: 98\n",
      "training D Loss: 0.01355061581747834\n",
      "training G Loss: 0.015882510045695703\n",
      "training Info Loss: 0.00013933744260520394\n",
      "D_real_acc: 0.9941578095305131\n",
      "D_fake_acc: 0.6328335796907628\n",
      "Epoch: 99\n",
      "training D Loss: 0.013422016513355737\n",
      "training G Loss: 0.015921764099005815\n",
      "training Info Loss: 0.00014166250322545954\n",
      "D_real_acc: 0.9949086131249854\n",
      "D_fake_acc: 0.6405762417587574\n",
      "Epoch: 100\n",
      "training D Loss: 0.013537113806830006\n",
      "training G Loss: 0.015884281238250166\n",
      "training Info Loss: 0.00014832612200737185\n",
      "D_real_acc: 0.9941108843058586\n",
      "D_fake_acc: 0.6366814481124329\n",
      "Epoch: 101\n",
      "training D Loss: 0.013335760879744833\n",
      "training G Loss: 0.0160189077017921\n",
      "training Info Loss: 0.00014744292958184728\n",
      "D_real_acc: 0.9948851505126581\n",
      "D_fake_acc: 0.6449402876516271\n",
      "Epoch: 102\n",
      "training D Loss: 0.013477854669291271\n",
      "training G Loss: 0.01598769185387605\n",
      "training Info Loss: 0.00014053580246408008\n",
      "D_real_acc: 0.9938293329579315\n",
      "D_fake_acc: 0.6381361300767228\n",
      "Epoch: 103\n",
      "training D Loss: 0.01342168436300799\n",
      "training G Loss: 0.0160334032672302\n",
      "training Info Loss: 0.0001457754170276005\n",
      "D_real_acc: 0.9942281973674949\n",
      "D_fake_acc: 0.6395204242040309\n",
      "Epoch: 104\n",
      "training D Loss: 0.01337705221173015\n",
      "training G Loss: 0.01601149930356772\n",
      "training Info Loss: 0.00014955212572385294\n",
      "D_real_acc: 0.9952840149222214\n",
      "D_fake_acc: 0.643555993524319\n",
      "Epoch: 105\n",
      "training D Loss: 0.01343588986817117\n",
      "training G Loss: 0.01609542404329063\n",
      "training Info Loss: 0.0001444256126101255\n",
      "D_real_acc: 0.9940874216935314\n",
      "D_fake_acc: 0.6380422796274137\n",
      "Epoch: 106\n",
      "training D Loss: 0.013316368374802484\n",
      "training G Loss: 0.016076053158859484\n",
      "training Info Loss: 0.00014250610867009875\n",
      "D_real_acc: 0.9942281973674949\n",
      "D_fake_acc: 0.6464184322282442\n",
      "Epoch: 107\n",
      "training D Loss: 0.01340328274035884\n",
      "training G Loss: 0.0161393192861549\n",
      "training Info Loss: 0.00014477646830060925\n",
      "D_real_acc: 0.9937589451209498\n",
      "D_fake_acc: 0.6413974331902114\n",
      "Epoch: 108\n",
      "training D Loss: 0.013360273383257298\n",
      "training G Loss: 0.016142324847675624\n",
      "training Info Loss: 0.00014811072534822117\n",
      "D_real_acc: 0.9951901644729124\n",
      "D_fake_acc: 0.6447056615283545\n",
      "Epoch: 109\n",
      "training D Loss: 0.013379618324473468\n",
      "training G Loss: 0.016151600634346053\n",
      "training Info Loss: 0.00015158989923997183\n",
      "D_real_acc: 0.9951197766359307\n",
      "D_fake_acc: 0.6444475727927548\n",
      "Epoch: 110\n",
      "training D Loss: 0.013282077948946007\n",
      "training G Loss: 0.01621632191574402\n",
      "training Info Loss: 0.00015383387711983374\n",
      "D_real_acc: 0.9943924356537857\n",
      "D_fake_acc: 0.6484596795007156\n",
      "Epoch: 111\n",
      "training D Loss: 0.013294261326573594\n",
      "training G Loss: 0.01632883733839474\n",
      "training Info Loss: 0.00014742546363714413\n",
      "D_real_acc: 0.9944393608784402\n",
      "D_fake_acc: 0.6465592079022078\n",
      "Epoch: 112\n",
      "training D Loss: 0.01328228874753181\n",
      "training G Loss: 0.016226174286206676\n",
      "training Info Loss: 0.00014707063653195766\n",
      "D_real_acc: 0.9946739870017127\n",
      "D_fake_acc: 0.6487177682363154\n",
      "Epoch: 113\n",
      "training D Loss: 0.013271405294050524\n",
      "training G Loss: 0.01633090651082018\n",
      "training Info Loss: 0.00014741713821128682\n",
      "D_real_acc: 0.9947678374510218\n",
      "D_fake_acc: 0.6470049975364257\n",
      "Epoch: 114\n",
      "training D Loss: 0.013228110960151305\n",
      "training G Loss: 0.01637238623073851\n",
      "training Info Loss: 0.00016030177145717647\n",
      "D_real_acc: 0.994603599164731\n",
      "D_fake_acc: 0.6488116186856244\n",
      "Epoch: 115\n",
      "training D Loss: 0.013239947194319849\n",
      "training G Loss: 0.016372702336317494\n",
      "training Info Loss: 0.00015680724333746497\n",
      "D_real_acc: 0.9948616879003308\n",
      "D_fake_acc: 0.6505478519978415\n",
      "Epoch: 116\n",
      "training D Loss: 0.013319982822222022\n",
      "training G Loss: 0.01641152797346423\n",
      "training Info Loss: 0.00015324711077404645\n",
      "D_real_acc: 0.9941343469181859\n",
      "D_fake_acc: 0.645573778184463\n",
      "Epoch: 117\n",
      "training D Loss: 0.013298379989207477\n",
      "training G Loss: 0.016364446517541306\n",
      "training Info Loss: 0.00014659522625560368\n",
      "D_real_acc: 0.9947443748386945\n",
      "D_fake_acc: 0.6430398160531193\n",
      "Epoch: 118\n",
      "training D Loss: 0.0132407377806171\n",
      "training G Loss: 0.016410937562893593\n",
      "training Info Loss: 0.00015333768759214397\n",
      "D_real_acc: 0.995237089697567\n",
      "D_fake_acc: 0.6472396236596982\n",
      "Epoch: 119\n",
      "training D Loss: 0.013213809579348609\n",
      "training G Loss: 0.01636195606539937\n",
      "training Info Loss: 0.000152008049651106\n",
      "D_real_acc: 0.9946270617770583\n",
      "D_fake_acc: 0.6503601510992234\n",
      "Epoch: 120\n",
      "training D Loss: 0.01321413750208928\n",
      "training G Loss: 0.016424840120782443\n",
      "training Info Loss: 0.00014410280021009478\n",
      "D_real_acc: 0.9936416320593134\n",
      "D_fake_acc: 0.6501489875882781\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "\n",
    "BCE_loss = nn.BCELoss().cuda()\n",
    "CE_loss = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# setup optimizer\n",
    "optimizerG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerInfo = optim.Adam(itertools.chain(G.parameters(), D.parameters()), \n",
    "                           lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "D_loss_list = []\n",
    "G_loss_list = []\n",
    "D_fake_acc_list = []\n",
    "D_real_acc_list = []\n",
    "Info_loss_list = []\n",
    "for epoch in range(120):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_G_loss = 0.0\n",
    "    D_fake_acc = 0.0\n",
    "    D_real_acc = 0.0\n",
    "    epoch_Info_loss = 0.0\n",
    "    total_length = len(train_X)\n",
    "    # shuffle\n",
    "    perm_index = torch.randperm(total_length)\n",
    "    train_X_sfl = train_X[perm_index]\n",
    "    # learning rate decay\n",
    "    if (epoch+1) == 50:\n",
    "        optimizerG.param_groups[0]['lr'] /= 2\n",
    "        optimizerD.param_groups[0]['lr'] /= 2\n",
    "        optimizerInfo.param_groups[0]['lr'] /= 2\n",
    "        print(\"learning rate change!\")\n",
    "\n",
    "    if (epoch+1) == 80:\n",
    "        optimizerG.param_groups[0]['lr'] /= 2\n",
    "        optimizerD.param_groups[0]['lr'] /= 2\n",
    "        optimizerInfo.param_groups[0]['lr'] /= 2\n",
    "        print(\"learning rate change!\")\n",
    "        \n",
    "    # construct training batch\n",
    "    for index in range(0,total_length ,BATCH_SIZE):\n",
    "        if index+BATCH_SIZE > total_length:\n",
    "            break\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        D.zero_grad()\n",
    "        input_X = train_X_sfl[index:index+BATCH_SIZE]\n",
    "        \n",
    "        #### train with real image -> ground truth = real label\n",
    "        real_image = Variable(input_X.cuda()) # use GPU \n",
    "        real_label = Variable(torch.ones((BATCH_SIZE))).cuda()\n",
    "        output, disc_output = D(real_image)\n",
    "        D_real_loss = BCE_loss(output, real_label)\n",
    "        D_real_acc += np.mean(((output > 0.5).cpu().data.numpy() == real_label.cpu().data.numpy()))\n",
    "        \n",
    "        #### train with fake image -> ground truth = fake label\n",
    "        z_ = torch.randn((BATCH_SIZE, 72,1,1))\n",
    "        c_1 = np.random.multinomial(1, 10 * [float(1.0 /10)], size=[BATCH_SIZE])\n",
    "        c_2 = np.random.multinomial(1, 10 * [float(1.0 /10)], size=[BATCH_SIZE])\n",
    "        c1 = np.concatenate((c_1,c_2),1)\n",
    "        c1 = torch.from_numpy(c1).type(torch.FloatTensor).view(BATCH_SIZE,20,1,1)\n",
    "        combine_input = Variable(torch.cat((z_, c1),1)).cuda()\n",
    "        \n",
    "        fake_image = G(combine_input)\n",
    "        fake_label = Variable(torch.zeros((BATCH_SIZE))).cuda()\n",
    "        output, disc_output = D(fake_image)\n",
    "        D_fake_loss = BCE_loss(output, fake_label)\n",
    "        D_fake_acc += np.mean(((output > 0.5).cpu().data.numpy() == fake_label.cpu().data.numpy()))\n",
    "        \n",
    "        # update D\n",
    "        D_train_loss = D_real_loss + D_fake_loss\n",
    "        epoch_D_loss+=(D_train_loss.data[0])\n",
    "        D_train_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #### train Generator\n",
    "        G.zero_grad()\n",
    "        # generate fake image\n",
    "        z_ = torch.randn((BATCH_SIZE, 72,1,1))\n",
    "        c_1 = np.random.multinomial(1, 10 * [float(1.0 /10)], size=[BATCH_SIZE])\n",
    "        c_2 = np.random.multinomial(1, 10 * [float(1.0 /10)], size=[BATCH_SIZE])\n",
    "        c1 = np.concatenate((c_1,c_2),1)\n",
    "        c1 = torch.from_numpy(c1).type(torch.FloatTensor).view(BATCH_SIZE,20,1,1)\n",
    "        combine_input = Variable(torch.cat((z_, c1),1)).cuda()\n",
    "        c1 = Variable(c1).cuda()\n",
    "        \n",
    "        fake_image = G(combine_input)\n",
    "        fake_label_for_G = Variable(torch.ones((BATCH_SIZE))).cuda()\n",
    "        output, disc_output = D(fake_image)\n",
    "        G_loss = BCE_loss(output, fake_label_for_G)\n",
    "        epoch_G_loss += (G_loss.data[0])\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # information loss\n",
    "        disc_loss = BCE_loss(disc_output,c1.view(BATCH_SIZE,-1))\n",
    "        epoch_Info_loss += (disc_loss.data[0])\n",
    "        disc_loss.backward()\n",
    "        optimizerInfo.step()\n",
    "        \n",
    "    print(\"training D Loss:\",epoch_D_loss/(total_length))\n",
    "    print(\"training G Loss:\", epoch_G_loss/(total_length))\n",
    "    print(\"training Info Loss:\", epoch_Info_loss/(total_length))\n",
    "    D_loss_list.append(epoch_D_loss/(total_length))\n",
    "    G_loss_list.append(epoch_G_loss/(total_length))\n",
    "    Info_loss_list.append(epoch_Info_loss/(total_length))\n",
    "    \n",
    "    print(\"D_real_acc:\", D_real_acc/(total_length/BATCH_SIZE))\n",
    "    print(\"D_fake_acc:\", D_fake_acc/(total_length/BATCH_SIZE))\n",
    "    \n",
    "    D_real_acc_list.append(D_real_acc/(total_length/BATCH_SIZE))\n",
    "    D_fake_acc_list.append(D_fake_acc/(total_length/BATCH_SIZE))\n",
    "    # evaluation\n",
    "    G.eval()\n",
    "    fixed_img_output = G(fixed_input)\n",
    "    G.train()\n",
    "    torchvision.utils.save_image(fixed_img_output.cpu().data, './InfoGAN_output2/figB_'+str(epoch+1)+'.jpg',nrow=10)\n",
    "    \n",
    "torch.save(G.state_dict(), \"./models/InfoG_model_2.pkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(D_real_acc_list, label = \"D real accuracy\")\n",
    "plt.plot(D_fake_acc_list, label = \"D fake accuracy\")\n",
    "plt.title(\"Discriminator Accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(D_loss_list, label=\"D loss\")\n",
    "plt.plot(G_loss_list, label=\"G loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"./GAN_output/fig2_2.jpg\")\n",
    "plt.savefig(\"./InfoGAN_output2/figB_2.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
