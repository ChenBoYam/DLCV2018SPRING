{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "from reader import readShortVideo\n",
    "from reader import getVideoList\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from os import listdir\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load features \n",
    "with open(\"../features/train_cut_features_350.pkl\", \"rb\") as f:\n",
    "    train_cut_features = pickle.load(f)\n",
    "with open(\"../features/train_cut_labels_350.pkl\", \"rb\") as f:\n",
    "    train_cut_labels = pickle.load(f)\n",
    "with open(\"../features/train_cut_lengths_350.pkl\", \"rb\") as f:\n",
    "    train_cut_lengths = pickle.load(f)\n",
    "    \n",
    "with open(\"../features/valid_cut_features_no_cut.pkl\", \"rb\") as f:\n",
    "    valid_cut_features = pickle.load(f)\n",
    "with open(\"../features/valid_cut_labels_no_cut.pkl\", \"rb\") as f:\n",
    "    valid_cut_labels = pickle.load(f)\n",
    "with open(\"../features/valid_cut_lengths_no_cut.pkl\", \"rb\") as f:\n",
    "    valid_cut_lengths = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = torch.load(\"../models/RNN_FC_model.pkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=512, n_layers=2, dropout=0.1):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.hidden_size =  hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=False,\n",
    "                           batch_first=True)\n",
    "        self.bn_0 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.fc_1 = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
    "        self.bn_1 = nn.BatchNorm1d(int(self.hidden_size/2))\n",
    "        self.fc_2 = nn.Linear(int(self.hidden_size), 11)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, padded_sequence, input_lengths, hidden=None):\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(padded_sequence, \n",
    "                                                         input_lengths, \n",
    "                                                         batch_first=True)\n",
    "        outputs, (hn,cn) = self.lstm(packed, hidden) # output: (seq_len, batch, hidden*n_dir)\n",
    "        \n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        cut_frame_prediction = []\n",
    "        for i in range(outputs.size(0)):\n",
    "            category = self.fc_2(outputs[i])\n",
    "            cut_frame_prediction.append(category)\n",
    "\n",
    "        category = torch.stack(cut_frame_prediction)\n",
    "        return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_pad(input_feature, input_lengths, input_labels):\n",
    "    perm_index = np.argsort(input_lengths)[::-1]\n",
    "    input_feature =  [input_feature[i] for i in perm_index]\n",
    "    input_labels =  [input_labels[i] for i in perm_index]\n",
    "    input_lengths = sorted(input_lengths, reverse=True)\n",
    "    input_feature = nn.utils.rnn.pad_sequence(input_feature, batch_first=True)\n",
    "    return input_feature, input_labels, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_batch_size = 32\n",
    "input_feature, input_labels, input_lengths = sort_pad(train_cut_features[:init_batch_size], \n",
    "                                                      train_cut_lengths[:init_batch_size],\n",
    "                                                      train_cut_labels[:init_batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = 1024*7*7\n",
    "with torch.no_grad():\n",
    "    model = seq2seq(feature_size,hidden_size=512).cuda()\n",
    "    model.eval()\n",
    "    init_batch_size = 32\n",
    "    \n",
    "    input_feature, input_labels, input_lengths = sort_pad(train_cut_features[:init_batch_size], \n",
    "                                                          train_cut_lengths[:init_batch_size],\n",
    "                                                          train_cut_labels[:init_batch_size])\n",
    "    output = model(input_feature.cuda(), \n",
    "               input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "    def forward(self, model_output, groundtruth, lengths):\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        batch_size = model_output.size()[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            sample_length = lengths[i]\n",
    "            target = groundtruth[i].type(torch.LongTensor).cuda()\n",
    "            prediction = model_output[i][:sample_length]\n",
    "            partial_loss = criterion(prediction, target)\n",
    "            loss += partial_loss\n",
    "        loss = loss / batch_size\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "training loss 8.862705945968628\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "validation accuracy:  0.4660878112712975\n",
      "Epoch: 2\n",
      "training loss 7.444426536560059\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "validation accuracy:  0.4660878112712975\n",
      "Epoch: 3\n",
      "training loss 6.963853359222412\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "validation accuracy:  0.4660878112712975\n",
      "Epoch: 4\n",
      "training loss 6.72320294380188\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "validation accuracy:  0.4660878112712975\n",
      "Epoch: 5\n",
      "training loss 6.311319589614868\n",
      "0\n",
      "0\n",
      "14\n",
      "0\n",
      "59\n",
      "validation accuracy:  0.4754259501965924\n",
      "Epoch: 6\n",
      "training loss 6.050694704055786\n",
      "206\n",
      "0\n",
      "39\n",
      "85\n",
      "75\n",
      "validation accuracy:  0.49459370904325034\n",
      "Epoch: 7\n",
      "training loss 5.754278302192688\n",
      "869\n",
      "107\n",
      "183\n",
      "197\n",
      "97\n",
      "validation accuracy:  0.5371887287024901\n",
      "Epoch: 8\n",
      "training loss 5.453190445899963\n",
      "812\n",
      "102\n",
      "164\n",
      "190\n",
      "97\n",
      "validation accuracy:  0.5448885976408913\n",
      "Epoch: 9\n",
      "training loss 5.083771347999573\n",
      "855\n",
      "101\n",
      "170\n",
      "201\n",
      "96\n",
      "validation accuracy:  0.5447247706422018\n",
      "Epoch: 10\n",
      "training loss 4.7700968980789185\n",
      "827\n",
      "82\n",
      "146\n",
      "202\n",
      "93\n",
      "validation accuracy:  0.5504587155963303\n",
      "Epoch: 11\n",
      "training loss 4.498388409614563\n",
      "767\n",
      "4\n",
      "99\n",
      "192\n",
      "99\n",
      "validation accuracy:  0.5707732634338138\n",
      "Epoch: 12\n",
      "training loss 4.1024667620658875\n",
      "843\n",
      "39\n",
      "155\n",
      "214\n",
      "191\n",
      "validation accuracy:  0.5779816513761468\n",
      "Epoch: 13\n",
      "training loss 3.8429731130599976\n",
      "730\n",
      "7\n",
      "111\n",
      "166\n",
      "125\n",
      "validation accuracy:  0.5717562254259502\n",
      "Epoch: 14\n",
      "training loss 3.5350098609924316\n",
      "797\n",
      "43\n",
      "174\n",
      "185\n",
      "179\n",
      "validation accuracy:  0.5697903014416776\n",
      "Epoch: 15\n",
      "training loss 3.269717216491699\n",
      "920\n",
      "128\n",
      "294\n",
      "227\n",
      "302\n",
      "validation accuracy:  0.5704456094364351\n",
      "Epoch: 16\n",
      "training loss 3.0145612955093384\n",
      "791\n",
      "80\n",
      "215\n",
      "211\n",
      "221\n",
      "validation accuracy:  0.5730668414154653\n",
      "Epoch: 17\n",
      "training loss 2.843669831752777\n",
      "582\n",
      "52\n",
      "151\n",
      "185\n",
      "149\n",
      "validation accuracy:  0.5542267365661862\n",
      "Epoch: 18\n",
      "training loss 2.620392918586731\n",
      "838\n",
      "124\n",
      "275\n",
      "237\n",
      "254\n",
      "validation accuracy:  0.5668414154652687\n",
      "Epoch: 19\n",
      "training loss 2.399291157722473\n",
      "954\n",
      "151\n",
      "332\n",
      "270\n",
      "323\n",
      "validation accuracy:  0.5666775884665793\n",
      "Epoch: 20\n",
      "training loss 2.2586407363414764\n",
      "797\n",
      "114\n",
      "264\n",
      "225\n",
      "237\n",
      "validation accuracy:  0.5670052424639581\n"
     ]
    }
   ],
   "source": [
    "feature_size = 1024*7*7\n",
    "model = seq2seq(feature_size,hidden_size=512,dropout=0.5, n_layers=2).cuda()\n",
    "# model.load_state_dict(weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "BATCH_SIZE = 32\n",
    "loss_function = Loss().cuda()\n",
    "max_accuracy = 0\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    CE_loss = 0.0\n",
    "    total_length = len(train_cut_features)\n",
    "    # shuffle\n",
    "    perm_index = np.random.permutation(total_length)\n",
    "    train_X_sfl = [train_cut_features[i] for i in perm_index]\n",
    "    train_y_sfl = [train_cut_labels[i] for i in perm_index]\n",
    "    train_lengths_sfl = np.array(train_cut_lengths)[perm_index]\n",
    "    # construct training batch\n",
    "    for index in range(0,total_length ,BATCH_SIZE):\n",
    "        if index+BATCH_SIZE > total_length:\n",
    "            break\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        input_X = train_X_sfl[index:index+BATCH_SIZE]\n",
    "        input_y = train_y_sfl[index:index+BATCH_SIZE]\n",
    "        input_lengths = train_lengths_sfl[index:index+BATCH_SIZE]\n",
    "        input_X, input_y, input_lengths = sort_pad(input_X, input_lengths, input_y)\n",
    "        # use GPU\n",
    "#         input_X = torch.nn.utils.rnn.pad_sequence(input_X).cuda()\n",
    "        # forward + backward + optimize\n",
    "        output = model(input_X.cuda(), input_lengths)\n",
    "        # compute loss for each sample in training data\n",
    "        loss = loss_function(output, input_y,input_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        CE_loss += loss.cpu().data.numpy()\n",
    "    print(\"training loss\",CE_loss)\n",
    "    \n",
    "    # validation\n",
    "    same_difference = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_output = []\n",
    "        valid_y_list = []\n",
    "        for valid_X, valid_y, valid_lengths in zip(valid_cut_features, valid_cut_labels, \n",
    "                                                   valid_cut_lengths):\n",
    "            input_valid_X = valid_X.unsqueeze(0)\n",
    "            output = model(input_valid_X.cuda(), [valid_lengths])\n",
    "            prediction = torch.argmax(torch.squeeze(output.cpu()),1).data.numpy()\n",
    "            print(sum(prediction!=0))\n",
    "            valid_gt = np.array(valid_y)\n",
    "            same_difference.append(prediction==valid_gt)\n",
    "\n",
    "        total = []\n",
    "        for i in same_difference:\n",
    "            total+=list(i)\n",
    "        accuracy = np.mean(total)\n",
    "        print(\"validation accuracy: \",accuracy)\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"../models/RNN_seq2seq_model.pkt\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 1024*7*7\n",
    "model = seq2seq(feature_size,hidden_size=512,dropout=0.5, n_layers=2).cuda()\n",
    "model.load_state_dict(torch.load(\"../models/RNN_seq2seq_model.pkt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    valid_output = []\n",
    "    for valid_X, valid_lengths in zip(valid_cut_features, valid_cut_lengths):\n",
    "        input_valid_X = valid_X.unsqueeze(0)\n",
    "        output = model(input_valid_X.cuda(), [valid_lengths])\n",
    "        prediction = torch.argmax(torch.squeeze(output.cpu()),1).data.numpy()\n",
    "        valid_output.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dir_name = sorted(listdir(\"../HW5_data/FullLengthVideos/videos/valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_folder = \"../output/\"\n",
    "for i in range(len(valid_dir_name)):\n",
    "    with open(os.path.join(output_folder, valid_dir_name[i]+'.txt'), \"w\") as f:\n",
    "        for j, pred in enumerate(valid_output[i]):\n",
    "            f.write(str(pred))\n",
    "            if j != len(valid_output[i])-1:\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5wAAADuCAYAAABVsAiGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAMoXACAAAwxP8DsdvWKi7cGTsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reference: https://matplotlib.org/2.0.2/examples/api/colorbar_only.html\n",
    "video_num = 4\n",
    "test = valid_output[video_num][544:844]\n",
    "answer = valid_cut_labels[video_num][544:844]\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = plt.subplot(211)\n",
    "\n",
    "# colors = [\"wheat\", \"turqoise\", \"teal\", \"sienna\", \"salmon\", \"orange\", \n",
    "#           \"lightblue\", \"lavender\", \"gold\", \"darkblue\", \"azure\"]\n",
    "colors = plt.cm.get_cmap('tab20',11).colors\n",
    "cmap = matplotlib.colors.ListedColormap([colors[idx] for idx in test])\n",
    "# cmap = plt.cm.get_cmap(\"tab20\", 11)\n",
    "bounds = [i for i in range(len(test))]\n",
    "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "cb1 = matplotlib.colorbar.ColorbarBase(ax, cmap=cmap,\n",
    "                                       norm=norm,\n",
    "                                       boundaries=bounds,\n",
    "                                       spacing='proportional',\n",
    "                                       orientation='horizontal',\n",
    "                                       ticks= range(544,844))\n",
    "ax.set_ylabel('Prediction')\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "cmap = matplotlib.colors.ListedColormap([colors[idx] for idx in answer])\n",
    "bounds = [i for i in range(len(test))]\n",
    "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "cb2 = matplotlib.colorbar.ColorbarBase(ax2, cmap=cmap,\n",
    "                                       norm=norm,\n",
    "                                       boundaries=bounds,\n",
    "                                       spacing='proportional',\n",
    "                                       orientation='horizontal',\n",
    "                                       ticks= list(range(544,844)))\n",
    "\n",
    "\n",
    "ax2.set_ylabel('GroundTruth')\n",
    "\n",
    "plt.savefig(\"temporal_action_segmentation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
